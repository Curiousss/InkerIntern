{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIP Project\n",
    "\n",
    "\n",
    "### Problem Statement: Use only Winograd Conv and convert this into Tutorial (train as well)\n",
    "### https://github.com/zlpure/Facial-Expression-Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the final code lets dive into the Winograd Convolutions.\n",
    "\n",
    "### Winograd Convolutions:\n",
    "\n",
    "Inspite of reading many papers on Winograd algorithm I still don't get how its works. These are the sourcs I refered to \n",
    "- https://arxiv.org/pdf/1509.09308.pdf\n",
    "- https://www.scribd.com/doc/55802885/Winograd-algorithm\n",
    "\n",
    "What I've gathered from all these sources has been summerized here:\n",
    "\n",
    "#### What are Winograd Convolutions:\n",
    "Winograd Convolutions uses the Winograd minimal filtering algorithm. The key idea is to perform convolution in transformed domains using Winograd algorithm. This algorithm reduces the number of multiplications with the expense of additional addition and constant multiplication.\n",
    "According to the article https://ai.intel.com/winograd-2/, The Winograd algorithm works on small tiles of the input image. In a nutshell, the input tile and filter are transformed, the outputs of the transform are multiplied together in an element-wise fashion, and the result is transformed back to obtain the outputs of the convolution.\n",
    "\n",
    "#### Why do we need Winograd Conv? \n",
    "In most of the applicaions of deep neural networks, the speed takes the priority over precision. E.g. in self-driving cars.\n",
    "\n",
    "Other references:\n",
    "- http://cs231n.stanford.edu/reports/2016/pdfs/117_Report.pdf\n",
    "- https://arxiv.org/pdf/1803.09004.pdf\n",
    "- https://www.encyclopediaofmath.org/index.php/Winograd_small_convolution_algorithm\n",
    "\n",
    "#### Where to find the Winograd Conv?\n",
    "Since I was not confident of my understanding in Winograd algorithms I couldn't implement it. There were few implementations of the algorithm. One was Nervana Neon :\n",
    "https://github.com/NervanaSystems/neon/tree/master/neon\n",
    "\n",
    "Then there is CudNN which implements Wino Conv, also there is an environment variable TF_ENABLE_WINOGRAD_NONFUSED, that could be used to enable or disable it. So I chose this to find out how Wino Conv can help.\n",
    "https://docs.nvidia.com/deeplearning/dgx/tensorflow-user-guide/index.html#tf_enable_winograd_nonfused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Winograd Convolutions in CuDNN \n",
    "I used Google Colaboratory, which did not have Cuda and CuDNN installed. Although I was expecting Winograd Conv to speed up the training, the prep took a lot of time. Everytime I log in I had to install Cuda, and CuDNN that took a lot of time. \n",
    "I tested the speed for just one epoch, supposing that it should be enough to measure the difference when Wino-Conv is on and off.\n",
    "I did not find much difference in either case.\n",
    "\n",
    "The set-up for Cuda 8 CuDNN 6\n",
    "https://github.com/Curiousss/InkerIntern/blob/master/FER_WINO_SEPARABLE.ipynb\n",
    "\n",
    "This is the set-up for Cuda 9 CuDNN 7\n",
    "https://github.com/Curiousss/InkerIntern/blob/master/CUda9Cudnn7.ipynb\n",
    "\n",
    "I think I spent almost 80-90% of the time on Wino-Conv so it was very difficult to let go since I felt I did not go anywhere with it. First of all I did not get the math, and then implementation did not seem to improve the speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Facial Emotion Recognition\n",
    "First I ran implementation given in https://github.com/zlpure/Facial-Expression-Recognition using the model.json and weights given.\n",
    "\n",
    "I implemented the same model at first. Then enhanced with these changes:\n",
    "\n",
    "- Separable Convolutions: To speed up the model the regular convolutions was replaced with SeparableConv2D. The speed almost doubled.\n",
    "- The model with 7x7 and 5x5 layers did not help in any kind of improvement. Only 3x3 convolutions were retained. Since two 3x3 conv layers have a receptive field of 5x5, and have fewer mathematical operations and more non-linearities. So they should be faster and able to create more complex functions.\n",
    "- Global Average Pooling: The fully-connected layers were replaced with Global Average Pooling. That increased the accuracy and the speed. GAP helps in minimizing overfitting reducing the total number of parameters in the model. \n",
    "- Image Augmentation:\n",
    "- The Dropout layers were removed since ImageAugmentation and GAP are already regularizing the network.\n",
    "- Small batches can oﬀer a regularizing eﬀect (Wilson and Martinez, 2003),perhaps due to the noise they add to the learning process. Generalizationerror is often best for a batch size of 1. Training with such a small batchsize might require a small learning rate to maintain stability because of thehigh variance in the estimate of the gradient. The total runtime can be veryhigh as a result of the need to make more steps, both because of the reducedlearning rate and because it takes more steps to observe the entire training set.\n",
    "\n",
    "\n",
    "https://github.com/Curiousss/InkerIntern/blob/master/FER_WINO_SEPARABLE_NO_CUDNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "FER_WINO_SEPARABLE_NO_CUDNN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
