{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIP Project\n",
    "\n",
    "\n",
    "### Problem Statement: Use only Winograd Conv and convert this into Tutorial (train as well)\n",
    "### https://github.com/zlpure/Facial-Expression-Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the Facial Expression Recognition let's discuss about the Winograd Convolutions.\n",
    "\n",
    "### Winograd Convolutions:\n",
    "\n",
    "Inspite of reading many papers on Winograd algorithm I still don't get how its works. These are the sources I refered to \n",
    "- https://arxiv.org/pdf/1509.09308.pdf\n",
    "- https://www.scribd.com/doc/55802885/Winograd-algorithm\n",
    "\n",
    "What I've gathered from all these sources has been summerized here:\n",
    "\n",
    "#### What are Winograd Convolutions:\n",
    "Winograd Convolutions uses the Winograd minimal filtering algorithm. The key idea is to perform convolution in transformed domains using Winograd algorithm. This algorithm reduces the number of multiplications with the expense of additional addition and constant multiplication.\n",
    "According to the article https://ai.intel.com/winograd-2/, The Winograd algorithm works on small tiles of the input image. In a nutshell, the input tile and filter are transformed, the outputs of the transform are multiplied together in an element-wise fashion, and the result is transformed back to obtain the outputs of the convolution.\n",
    "\n",
    "#### Why do we need Winograd Conv? \n",
    "In most of the applicaions of deep neural networks, the speed takes the priority over precision. E.g. in self-driving cars. Hence fast algorithms like Winograd Convolutions are used.\n",
    "\n",
    "Other references:\n",
    "- http://cs231n.stanford.edu/reports/2016/pdfs/117_Report.pdf\n",
    "- https://arxiv.org/pdf/1803.09004.pdf\n",
    "- https://www.encyclopediaofmath.org/index.php/Winograd_small_convolution_algorithm\n",
    "\n",
    "#### Where to find the Winograd Conv?\n",
    "Since I was not confident in my understanding of Winograd algorithms I couldn't implement it. There were few implementations of the algorithm. I found these options which were directly usable:\n",
    "- One is Nervana Neon: Implementation of Winograd Conv in this architecture seemed straight forward. But I did not find a way of comparing the speed with and without Winograd Conv with this implementation.\n",
    "https://github.com/NervanaSystems/neon/tree/master/neon\n",
    "\n",
    "- Then there is CudNN which implements Winograd Conv. The Winograd Conv is enabled by default in version higher than 5. But it  also provides an environment variable TF_ENABLE_WINOGRAD_NONFUSED, that could be used to enable or disable it. So I chose this to find out how Winograd Conv can help the performance of Deep neural networks.\n",
    "https://docs.nvidia.com/deeplearning/dgx/tensorflow-user-guide/index.html#tf_enable_winograd_nonfused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Winograd Convolutions in CuDNN \n",
    "I used Google Colaboratory, which did not have Cuda and CuDNN installed. Hence everytime I login to colab I had to install Cuda, and CuDNN that took a lot of time. \n",
    "I tested the speed for just one epoch, supposing that it should be enough to measure the difference when Wino-Conv is on and off.\n",
    "I did not find much difference in either case. \n",
    "\n",
    "This notebook installs CUDA 9 and CuDNN 7 in colaboratory.\n",
    "https://github.com/Curiousss/InkerIntern/blob/master/CUda9Cudnn7.ipynb\n",
    "\n",
    "Both version re-install tensorflow-gpu to make sure that the CuDNN is used. \n",
    "The model is tested with the environment variable TF_ENABLE_WINOGRAD_NONFUSED set to \"1\" and \"0\". As a result the speed was almost the same. Any small differences in the speed were not found related to the setting of the flag. Sometimes enabling the flag was faster sometime disabling was faster.\n",
    "\n",
    "\n",
    "This notebook installs CUDA 8 and CuDNN 6 in colaboratory.\n",
    "https://github.com/Curiousss/InkerIntern/blob/master/FER_WINO.ipynb\n",
    "\n",
    "I think I spent almost 80-90% of the project time on understanding and testing Winograd-Convolutions but I had to let go of it since it did not seem to improve the speed. Hence it has not been used in the Facial Expression Recognition model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Facial Emotion Recognition\n",
    "First I ran the implementation given in https://github.com/zlpure/Facial-Expression-Recognition using the model.json and weights given. The accuracy was 64% and with Image augmentation it went upto 66-67%. The speed was around 77s per epoch.\n",
    "Find the implementation here: https://github.com/Curiousss/InkerIntern/blob/master/FacialEmotion.ipynb\n",
    "\n",
    "I re-implemented the same model at first. Then enhanced with these features:\n",
    "\n",
    "- Separable Convolutions: To speed up the model the regular convolutions were replaced with SeparableConv2D. The speed almost doubled. They have fewer parameters than regular convolutional layers, and thus are less prone to overfitting. With fewer parameters, they also require less operations to compute, and thus are cheaper and faster.\n",
    "\n",
    "\n",
    "- The model with 7x7 and 5x5 layers did not help in any kind of improvement. Only 3x3 convolutions were retained. Two 3x3 conv layers have a receptive field of 5x5, and have fewer mathematical operations and more non-linearities. So they should be faster and able to create more complex functions.\n",
    "\n",
    "\n",
    "- Global Average Pooling: The fully-connected layers were replaced with Global Average Pooling. This increased the accuracy and the speed. GAP helps in minimizing overfitting reducing the total number of parameters in the model. To match the Global Average Pooling I tried using Average pooling in all other layers, but that did not improve the performance hence I retained the Max Pooling for the top layers.\n",
    "\n",
    "\n",
    "- Image Augmentation: Applying random transformation on the image might actually hinder the training process. Hence each transformation was tested for its efficiecy and then chosen for the final model. \n",
    "    - Applying horizontal flip did improve the accuracy. Applying vertical flip was not very helpful.\n",
    "    - Image shearing and zooming was applied.\n",
    "    - Width and height shift was applied.\n",
    "    - Normalization was applied outside the Augmentation function, so none of the normalization options in were applied during Image Generator.\n",
    "\n",
    "\n",
    "While trying to display the images that was read from the data, I noticed some of them were just zeros. In an attempt to find ways to clean the input data I found this list of bad data indices and used the same: https://github.com/LamUong/FacialExpressionRecognition/blob/master/badtrainingdata.txt\n",
    "\n",
    "### The accuracy achieved was 67%\n",
    "\n",
    "#### For neater code please refer to the code in my github profile: https://github.com/Curiousss/InkerIntern/blob/master/FER_WINO_SEPARABLE_NO_CUDNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 73,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "Q9eNowMGaO_J",
    "outputId": "a2de03f2-73fa-4a21-9905-dab09fffd980"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BEb9SWJUV8T2",
    "outputId": "5ea861d0-cdc0-4286-b93b-59361c0548e9"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "id": "ldEdUzQbuQIi",
    "outputId": "fa1f7c30-c8b0-4725-b742-3500ee1754bb"
   },
   "outputs": [],
   "source": [
    "!tar xvf fer2013.tar\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dTyUIGDeuAQf",
    "outputId": "47252ee3-568e-47dd-f89d-36be332f387d"
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D, InputLayer\n",
    "from keras.layers import Convolution2D, SeparableConv2D, MaxPooling2D, BatchNormalization \n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "OqJCQeGxhq4x"
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 64\n",
    "classes = 7\n",
    "epoch = 100\n",
    "img_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "sq7nNsVmhsYX"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('fer2013/fer2013.csv')\n",
    "csv_f = csv.reader(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yQSDhHZ3tKqk"
   },
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "val_x =[]\n",
    "val_y =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "eWmuGFqNF5Ds"
   },
   "outputs": [],
   "source": [
    "ToBeRemovedTrainingData = []\n",
    "with open(\"baddata.txt\", \"r\") as text:\n",
    "  for line in text:\n",
    "    ToBeRemovedTrainingData.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HmmgpkTpiWyS"
   },
   "outputs": [],
   "source": [
    "num=0\n",
    "for row in csv_f:\n",
    "  num = num +1\n",
    "  if num in ToBeRemovedTrainingData or num==1:\n",
    "    continue\n",
    "  #print(row)\n",
    "  #print(num)\n",
    "  temp_list = []\n",
    "  for pixel in row[1].split( ):\n",
    "    temp_list.append(int(pixel))\n",
    "\n",
    "  if str(row[2]) == \"Training\":\n",
    "    train_y.append(int(row[0]))\n",
    "    train_x.append(temp_list) \n",
    "  elif str(row[2]) == \"PublicTest\":\n",
    "    val_y.append(int(row[0]))\n",
    "    val_x.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "H6rDJXl4rUKS"
   },
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)\n",
    "val_x = np.asarray(val_x)\n",
    "val_y = np.asarray(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "lmXwV3InrVsQ"
   },
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(train_x.shape[0], 48, 48)\n",
    "train_x = train_x.reshape(train_x.shape[0], 48, 48, 1 )\n",
    "train_y = np_utils.to_categorical(train_y, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "s3iZUfL1ztRu"
   },
   "outputs": [],
   "source": [
    "val_x = val_x.reshape(val_x.shape[0], 48, 48)\n",
    "val_x = val_x.reshape(val_x.shape[0], 48, 48, 1)\n",
    "val_y = np_utils.to_categorical(val_y, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "FAWueVKKI0CR",
    "outputId": "691de95b-8fcc-4194-93a6-2b21d637e948"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#print(train_x.shape)\n",
    "\n",
    "showimg = train_x[1].reshape(48,48)\n",
    "img = Image.fromarray(showimg.astype('uint8'))\n",
    "from IPython.display import display\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bkb-p1BXzBrI"
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train_x = train_x.astype('float32')\n",
    "train_x = train_x / 255.0\n",
    "val_x = val_x.astype('float32')\n",
    "val_x = val_x / 255.0\n",
    "train_x = train_x - 0.5\n",
    "train_x = train_x * 2\n",
    "val_x = val_x - 0.5\n",
    "val_x = val_x * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jMKS1KOEGsB3"
   },
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, img_channels)\n",
    "model = Sequential()\n",
    "model.add(SeparableConv2D(filters=64, kernel_size=(3, 3), padding='same',\n",
    "                            name='image_array', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(SeparableConv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "#model.add(Dropout(.3))\n",
    "\n",
    "model.add(SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(SeparableConv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "#model.add(Dropout(.3))\n",
    "\n",
    "model.add(SeparableConv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(SeparableConv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "#model.add(Dropout(.3))\n",
    "\n",
    "model.add(SeparableConv2D(filters=512, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(SeparableConv2D(filters=512, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "model.add(InputLayer(input_shape=(3, 3, 1024)))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Hy0FyCcc2li0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "filepath='Model.best.hdf5'\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "rZYe0RPXn4O_"
   },
   "outputs": [],
   "source": [
    "model.load_weights('Model.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3692
    },
    "colab_type": "code",
    "id": "bJiUe6bEgjHD",
    "outputId": "d652e758-edf4-4c7b-8011-e0ab664055f5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2)  # randomly flip images\n",
    "\n",
    "datagen.fit(train_x)\n",
    "\n",
    "model.fit_generator(datagen.flow(train_x, train_y,\n",
    "                    batch_size=batch_size),\n",
    "                    steps_per_epoch=(train_x.shape[0]/batch_size),\n",
    "                    epochs=50,\n",
    "                    validation_data=(val_x, val_y),\n",
    "                    callbacks=[checkpointer])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 5209
    },
    "colab_type": "code",
    "id": "2Zgb4ojyDNYZ",
    "outputId": "47a7f747-79a3-4118-b86f-cb05be2142f4"
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "start_time  = time.time()\n",
    "model.fit(train_x, train_y, epochs=150, batch_size=batch_size, validation_data=(val_x, val_y),\n",
    "             callbacks=[checkpointer])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "id": "WMaPXKT8FSGf",
    "outputId": "a0d5cdf5-008e-4b13-cf13-11adae5e0d85"
   },
   "outputs": [],
   "source": [
    "def predict_emotion(model, pic):\n",
    "  pic = pic.convert('L')\n",
    "  pic = pic.resize((48,48))\n",
    "  \n",
    "  from IPython.display import display\n",
    "  display(pic)\n",
    "  pic_np=np.asarray(pic)#.getdata()).reshape(48, 48, 1)\n",
    "  pic_np = pic_np.reshape(1, 48, 48, 1)\n",
    "  print(pic_np.shape)\n",
    "  pic_np = pic_np / 255.0\n",
    "  pic_np = pic_np - 0.5\n",
    "  pic_np = pic_np * 2\n",
    "  \n",
    "  print(pic_np.shape)\n",
    "  y = model.predict(pic_np)\n",
    "  print(\"0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\")\n",
    "  print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "5AAzrfWFHGAP",
    "outputId": "5450a033-4373-4e4e-ce27-c7df896f16d9"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "colab_type": "code",
    "id": "2LmFcuMmHfbr",
    "outputId": "501e112b-6432-4e48-e550-b9a94dd29362",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "celebanger = Image.open(\"celeb_fer1.jpg\")\n",
    "predict_emotion(model, celebanger)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "FER_WINO_SEPARABLE_NO_CUDNN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
